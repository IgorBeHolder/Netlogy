{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Tokens: 200**************************************************\n",
      "Testing concurrency: 8\n",
      "Testing concurrency: 16\n",
      "Testing concurrency: 32\n",
      "Testing concurrency: 64\n",
      "Testing concurrency: 128\n",
      "     concurency  context_length max_tokens  total_response_time  \\\n",
      "0             8            5000        200            15.645329   \n",
      "1             8            5000        200            15.643634   \n",
      "2             8            5000        200            15.641177   \n",
      "3             8            5000        200            15.638423   \n",
      "4             8            5000        200            15.638022   \n",
      "..          ...             ...        ...                  ...   \n",
      "243         128            5000        200           216.836032   \n",
      "244         128            5000        200           212.989813   \n",
      "245         128            5000        200           216.835996   \n",
      "246         128            5000        200           216.834008   \n",
      "247         128            5000        200           216.835090   \n",
      "\n",
      "     completion_tokens  prompt_tokens  \n",
      "0                  200           4003  \n",
      "1                  195           4122  \n",
      "2                  200           4065  \n",
      "3                  155           3933  \n",
      "4                  183           4063  \n",
      "..                 ...            ...  \n",
      "243                171           3858  \n",
      "244                 30           3844  \n",
      "245                200           4158  \n",
      "246                104           4014  \n",
      "247                200           3824  \n",
      "\n",
      "[248 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "file_path = \"/Users/velo1/SynologyDrive/GIT_syno/Mac/Netology/NLP/data/book-war-and-peace.txt\"\n",
    "max_tokens = '200'\n",
    "approx_chars_per_token = 3\n",
    "seq_len = 5000\n",
    "res = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    long_text = file.read()\n",
    "\n",
    "def extract_random_sequence(text, seq_length):\n",
    "    seq_length *= approx_chars_per_token\n",
    "    # Choose a random starting index\n",
    "    start_index = random.randint(0, len(text) - seq_length)\n",
    "    # Extract the sequence from the text\n",
    "    sequence = text[start_index:start_index + seq_length]\n",
    "    return sequence.strip()\n",
    "\n",
    "\n",
    "async def measure_llm_response_time(context, session, concurency):\n",
    "    '''Asynchronous function to measure the response time for a given context length'''\n",
    "    data = {\n",
    "        \"model\": \"/model-store/mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        \"messages\": [\n",
    "            {\"content\": \"<s>[INST]You are a helpful assistant. Analyze the context and answer users question.[/INST]\", \"role\": \"system\"},\n",
    "            {\"content\": f\"The context: {context}</s>\", \"role\": \"assistant\"},\n",
    "            {\"content\": \"Try to count all the unique names in the context. What are these names and their count? Try to do your best and make your answer as detailed as possible\", \"role\": \"user\"}\n",
    "        ],\n",
    "        \"max_tokens\": f'{max_tokens}'\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    async with session.post('http://194.135.112.219:3003/v1/chat/completions', json=data, headers=headers) as response:\n",
    "        response_data = await response.json()\n",
    "    total_response_time = time.time() - start_time\n",
    "    usage_data = response_data['usage']\n",
    "    completion_tokens = usage_data['completion_tokens']\n",
    "    prompt_tokens = usage_data['prompt_tokens']\n",
    "\n",
    "\n",
    "    return {\n",
    "        'concurency': concurency,\n",
    "        'context_length': seq_len,\n",
    "        'max_tokens': max_tokens,\n",
    "        'total_response_time': total_response_time,\n",
    "        'completion_tokens': completion_tokens,\n",
    "        'prompt_tokens': prompt_tokens\n",
    "    }\n",
    "\n",
    "async def run_experiment(full_text, concurrency):\n",
    "    '''Asynchronous function to run the experiment with a given concurrency'''\n",
    "    start_power = np.log10(200)\n",
    "    end_power = np.log10(200) + 1\n",
    "    context_lengths = np.logspace(start_power, end_power, num=1, dtype=int)\n",
    "    results = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for length in context_lengths:\n",
    "            for _ in range(concurrency):\n",
    "                # Generate a unique random sequence for each task\n",
    "                unique_text = extract_random_sequence(full_text, seq_len)\n",
    "                tasks.append(measure_llm_response_time(unique_text, session, concurrency))\n",
    "                # tasks.append(measure_llm_response_time(full_text[:seq_len*approx_chars_per_token], session, concurrency))\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response in responses:\n",
    "            results.append(response )\n",
    "            # print(response)\n",
    "            # print(f\"Context Length: {response['context_length']} Tokens - Total: {response['total_response_time']:.4f} sec\")\n",
    "    return results\n",
    "\n",
    "async def main():\n",
    "    global res\n",
    "    for max_tokens in ['200']:  # Add other values as needed\n",
    "        print(f\"Max Tokens: {max_tokens}{'*'*50}\")\n",
    "        for concurrency in [8, 16, 32, 64, 128]:  # Add other concurrency levels as needed    \n",
    "            print(f\"Testing concurrency: {concurrency}\")\n",
    "            # Pass the full `long_text` to extract a random sequence for each experiment\n",
    "            res += await run_experiment(long_text, concurrency)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "await main()\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(res)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n",
    "df.to_csv('/Users/velo1/SynologyDrive/GIT_syno/Mac/Netology/NLP/LLM_batching_3090.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
