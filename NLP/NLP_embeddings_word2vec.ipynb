{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypt7ymI8X-rM"
   },
   "source": [
    "# Embeddings  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1VGqw8yMPsFmlH5IHv8Vca31OVaikcgrh?usp=sharing)\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Векторные модели, которые мы рассматривали до этого (tf-idf, BOW), условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. \n",
    "\n",
    "Другой класс моделей, который более повсевмёстно распространён на сегодняшний день, называется *предсказательными* (или *нейронными*) моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей слов. Одной из самых известных таких моделей является word2vec. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook). Вот две самые главные статьи:\n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "\n",
    "Полученные таким образом вектора называются *Distributed Representations of Words (распределенными представлениями слов)*, или **эмбеддингами**.\n",
    "\n",
    "\n",
    "### Как это обучается?\n",
    "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW).  \n",
    "\n",
    "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
    "\n",
    "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
    "\n",
    "### Как это работает?\n",
    "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
    "\n",
    "\n",
    "С помощью дистрибутивных векторных моделей можно строить семантические пропорции (они же аналогии) и решать примеры:\n",
    "\n",
    "* *король: мужчина = королева: женщина* \n",
    " $\\Rightarrow$ \n",
    "* *король - мужчина + женщина = королева*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3sSyfHaX-rN"
   },
   "source": [
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oH_UDyFX-rO"
   },
   "source": [
    "### Проблемы\n",
    "Невозможно установить тип семантических отношений между словами: синонимы, антонимы и т.д. будут одинаково близки, потому что обычно употребляются в схожих контекстах. Поэтому близкие в векторном пространстве слова называют *семантическими ассоциатами*. Это значит, что они семантически связаны, но как именно — непонятно.\n",
    "\n",
    "\n",
    "### RusVectōrēs\n",
    "\n",
    "\n",
    "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятора семантической близости».\n",
    "\n",
    "\n",
    "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/) (о них чуть дальше).\n",
    "\n",
    "### Визуализация\n",
    "А [вот тут](https://projector.tensorflow.org/) есть хорошая визуализация для английского."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXNm8flVX-rP"
   },
   "source": [
    "## Gensim\n",
    "\n",
    "Использовать предобученную модель эмбеддингов или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "### Как использовать готовую модель\n",
    "\n",
    "Модели word2vec бывают разных форматов:\n",
    "\n",
    "* .vec.gz — обычный файл\n",
    "* .bin.gz — бинарник\n",
    "\n",
    "Загружаются они с помощью одного и того же класса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
    "\n",
    "Если же эмбеддинги обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. для загрузки предобученных эмбеддингов *glove, fasttext, bpe* и любых других нужна именно она.\n",
    "\n",
    "Скачаем с RusVectōrēs модель для русского языка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "sTLnNKaNX-rP",
    "outputId": "2a1f013e-b57d-4984-9c73-bc64e5a0e816"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/velo1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data \n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors  # module provides a way to work with pre-trained word embeddings\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from beholder import print_methods, call_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model ['ruscorpora_upos_cbow_300_20_2019'](https://rusvectores.org/ru/models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://vectors.nlpl.eu/repository/20/180.zip\n",
    "# local NLP folder\n",
    "nlp_dir ='/Users/velo1/SynologyDrive/GIT_syno/data/NLP/'\n",
    "permanent_id, file_name = ('ruscorpora_upos_cbow_300_20_2019', '180.zip')\n",
    "\n",
    "# Check if the target directory already exists (indicating unzipping has been done)\n",
    "target_dir = nlp_dir + permanent_id + '/'\n",
    "if not os.path.exists(target_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "    # Create a ZipFile object\n",
    "    with zipfile.ZipFile(nlp_dir + file_name, 'r') as zip_ref:\n",
    "        # Extract all the files to the specified directory\n",
    "        zip_ref.extractall(target_dir)\n",
    "\n",
    "# load the word2vec model\n",
    "filename = target_dir + 'model.bin'\n",
    "\n",
    "model_ru = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "zm9z6SN3X-rS",
    "outputId": "e3280992-dadc-40a2-c356-547cf399e599"
   },
   "outputs": [],
   "source": [
    "# Retrieve a URL into a temporary location on disk.\n",
    "# urllib.request.urlretrieve(\" http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")\n",
    "# urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/20/220.zip\", \"ruscorpora_upos_cbow_300_20_2019.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "mkuCawyFX-rW",
    "outputId": "654509e9-9186-4af1-bea2-ab5962202014"
   },
   "outputs": [],
   "source": [
    "# model_path = 'http://vectors.nlpl.eu/repository/20/180.zip'\n",
    "\n",
    "# model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "# model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_mystem2upos(text=[\"Текст нужно передать функции в виде list!\"]) -> list:\n",
    "    '''Converts POS-tagging from mystem format to UPOS format.'''\n",
    "    # \"https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map\"\n",
    "\n",
    "    mapping = {\n",
    "        \"A\": \"ADJ\",\n",
    "        \"ADV\": \"ADV\",\n",
    "        \"ADVPRO\": \"ADV\",\n",
    "        \"ANUM\": \"ADJ\",\n",
    "        \"APRO\": \"DET\",\n",
    "        \"COM\": \"ADJ\",\n",
    "        \"CONJ\": \"SCONJ\",\n",
    "        \"INTJ\": \"INTJ\",\n",
    "        \"NONLEX\": \"X\",\n",
    "        \"NUM\": \"NUM\",\n",
    "        \"PART\": \"PART\",\n",
    "        \"PR\": \"ADP\",\n",
    "        \"S\": \"NOUN\",\n",
    "        \"SPRO\": \"PRON\",\n",
    "        \"UNKN\": \"X\",    # X for all unknown tags\n",
    "        \"V\": \"VERB\",\n",
    "    }\n",
    "    tagged = []\n",
    "    for w in text:\n",
    "        try:\n",
    "            pos = w.split(\"_\")[1].strip()\n",
    "            if pos in mapping:\n",
    "                pos = mapping[pos]  # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                pos = \"X\"  # на случай, если попадется тэг, которого нет в маппинге\n",
    "            tagged.append(w.split(\"_\")[0] + \"_\" + pos)\n",
    "        except KeyError:\n",
    "            continue  # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['error_X']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_mystem2upos(['мама_S', 'мыть_V', 'рама_S', 'error_e']) \n",
    "tag_mystem2upos(['error_e']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvvO1ew5p3PP"
   },
   "source": [
    "Возьмем несколько слов для примера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H4D0hMgkX-rY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'биткоин_NOUN']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']\n",
    "tag_mystem2upos(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbTShU9aX-ra"
   },
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных). **NB!** В названиях моделей на `rusvectores` указано, какой тегсет они используют (mystem, upos и т.д.)\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Uer5sHyX-ra",
    "outputId": "901d7be2-77e6-4906-a5f8-672e7efa1d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN shape: (300,). First 10 values: [ 1.805067   -0.877623   -1.0102742   2.8518744  -0.43311968 -3.7207692\n",
      " -3.4317713  -0.7634762  -4.9961104  -1.1313324 ]\n",
      "\n",
      "10 nearest neighbours of \"день_NOUN\":\n",
      "неделя_NOUN :  0.7375996112823486\n",
      "день_PROPN :  0.706766664981842\n",
      "месяц_NOUN :  0.7037326097488403\n",
      "час_NOUN :  0.6643950939178467\n",
      "утро_NOUN :  0.6526744961738586\n",
      "вечер_NOUN :  0.6038411259651184\n",
      "сутки_NOUN :  0.5923080444335938\n",
      "воскресенье_NOUN :  0.5842781066894531\n",
      "полдень_NOUN :  0.5743688344955444\n",
      "суббота_NOUN :  0.5345946550369263\n",
      "\n",
      "\n",
      "ночь_NOUN shape: (300,). First 10 values: [-0.10776415  0.32673436  0.52870405  2.1667976   0.7689093  -2.4214501\n",
      " -1.4222336  -2.972895    0.18769576 -0.05231643]\n",
      "\n",
      "10 nearest neighbours of \"ночь_NOUN\":\n",
      "ночь_PROPN :  0.8310787081718445\n",
      "вечер_NOUN :  0.7183678150177002\n",
      "рассвет_NOUN :  0.6965947151184082\n",
      "ночи_NOUN :  0.692021906375885\n",
      "полночь_NOUN :  0.6704976558685303\n",
      "ночь_VERB :  0.6615265011787415\n",
      "утро_NOUN :  0.6263936161994934\n",
      "ночной_ADJ :  0.6024709343910217\n",
      "полдень_NOUN :  0.5835085511207581\n",
      "сумерки_NOUN :  0.5671443939208984\n",
      "\n",
      "\n",
      "человек_NOUN shape: (300,). First 10 values: [ 0.02881786 -0.7942778   2.4604542   2.2049303   1.4084865   2.3384094\n",
      " -5.0771294   1.3228273  -0.7700866  -1.2985276 ]\n",
      "\n",
      "10 nearest neighbours of \"человек_NOUN\":\n",
      "человек_PROPN :  0.7850059270858765\n",
      "человеческий_ADJ :  0.5915265679359436\n",
      "существо_NOUN :  0.5736929774284363\n",
      "народ_NOUN :  0.5354466438293457\n",
      "личность_NOUN :  0.5296981930732727\n",
      "человечество_NOUN :  0.5282931327819824\n",
      "человкъ_PROPN :  0.5047001242637634\n",
      "индивидуум_NOUN :  0.5000404119491577\n",
      "нравственный_ADJ :  0.4972919225692749\n",
      "потому_ADV :  0.49293625354766846\n",
      "\n",
      "\n",
      "семантика_NOUN shape: (300,). First 10 values: [ 0.5002659  -1.3545024  -0.55890405 -1.0640687   0.9918463   0.10782916\n",
      "  0.32927004  1.5328914  -2.6711333   0.43661344]\n",
      "\n",
      "10 nearest neighbours of \"семантика_NOUN\":\n",
      "семантический_ADJ :  0.8019332885742188\n",
      "синтаксический_ADJ :  0.7569340467453003\n",
      "модальный_ADJ :  0.7296056747436523\n",
      "семантически_ADV :  0.7209396958351135\n",
      "смысловой_ADJ :  0.7159028053283691\n",
      "референция_NOUN :  0.7135109305381775\n",
      "ноэтический_ADJ :  0.7080267071723938\n",
      "языковой_ADJ :  0.7067197561264038\n",
      "лингвистический_ADJ :  0.692865788936615\n",
      "предикат_NOUN :  0.6877546906471252\n",
      "\n",
      "\n",
      "Увы, слова \"биткоин_NOUN\" нет в модели!\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    # convert POS-tagging from mystem format to UPOS format\n",
    "    word = tag_mystem2upos(w.split())[0] # index 0 to get string from list\n",
    "    # есть ли слово в модели? \n",
    "    if word in model_ru:\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(f'{word} shape: {model_ru[word].shape}. First 10 values: {model_ru[word][:10]}')\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        print(f'\\n10 nearest neighbours of \"{word}\":')\n",
    "        \n",
    "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(word, ': ', sim)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print('Увы, слова \"%s\" нет в модели!' % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GqDmAcJX-rc"
   },
   "source": [
    "### Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22025342\n"
     ]
    }
   ],
   "source": [
    "list_ = tag_mystem2upos(['человек_S', 'обезьяна_S'])\n",
    "print(model_ru.similarity(*list_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0w4pQooX-rf"
   },
   "source": [
    "Что получится, если вычесть из пиццы Италию и прибавить Германию?\n",
    "\n",
    "* positive — вектора, которые мы складываем\n",
    "* negative — вектора, которые вычитаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('чипсы_NOUN', 0.5982315540313721), ('гамбургер_NOUN', 0.5845527052879333)]\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.most_similar(positive=['пицца_NOUN', 'германия_PROPN'], negative=['италия_PROPN'])[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out which word doesn't go with the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "DrN2Jc31X-rh",
    "outputId": "295b7a9c-3a2a-4a80-fde1-d78df9c81e0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ананас_NOUN'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.doesnt_match(tag_mystem2upos(['пицца_S', 'пельмень_S', 'хот-дог_S', 'ананас_S']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 methods for (<class 'gensim.models.keyedvectors.KeyedVectors'>): \n",
      " ['add_lifecycle_event', 'add_vector', 'add_vectors', 'allocate_vecattrs', 'closer_than', 'cosine_similarities', 'distance', 'distances', 'doesnt_match', 'evaluate_word_analogies', 'evaluate_word_pairs', 'expandos', 'fill_norms', 'get_index', 'get_mean_vector', 'get_normed_vectors', 'get_vecattr', 'get_vector', 'has_index_for', 'index2entity', 'index2word', 'index_to_key', 'init_sims', 'intersect_word2vec_format', 'key_to_index', 'lifecycle_events', 'load', 'load_word2vec_format', 'log_accuracy', 'log_evaluate_word_pairs', 'mapfile_path', 'most_similar', 'most_similar_cosmul', 'most_similar_to_given', 'n_similarity', 'next_index', 'norms', 'rank', 'rank_by_centrality', 'relative_cosine_similarity', 'resize_vectors', 'save', 'save_word2vec_format', 'set_vecattr', 'similar_by_key', 'similar_by_vector', 'similar_by_word', 'similarity', 'similarity_unseen_docs', 'sort_by_descending_frequency', 'unit_normalize_all', 'vector_size', 'vectors', 'vectors_for_all', 'vectors_norm', 'vocab', 'wmdistance', 'word_vec', 'words_closer_than']\n"
     ]
    }
   ],
   "source": [
    "meths = print_methods(model_ru)\n",
    "# call_methods(model_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9XsGSE_HuFJ"
   },
   "source": [
    "**Упражнения для разминки**\n",
    "\n",
    "Найдите пример многозначного слова, для которого в топ-10 (метод `most_similar`) похожих на него слов входят слова связанные с разными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MWXGH79-H1ED"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('знак_ADV', 0.6484130620956421),\n",
       " ('знак_PROPN', 0.5194986462593079),\n",
       " ('символ_NOUN', 0.47708243131637573),\n",
       " ('значок_NOUN', 0.4415062963962555),\n",
       " ('эмблема_NOUN', 0.4384552240371704),\n",
       " ('символический_ADJ', 0.4278099834918976),\n",
       " ('надпись_NOUN', 0.4255180358886719),\n",
       " ('нарукавный_ADJ', 0.42526695132255554),\n",
       " ('кабалистический_ADJ', 0.4215017259120941),\n",
       " ('обозначение_NOUN', 0.4212318956851959)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.most_similar(positive=['знак_NOUN'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('руль_VERB', 0.5641716718673706),\n",
       " ('вираж_NOUN', 0.5314818024635315),\n",
       " ('пропеллер_NOUN', 0.5066705942153931),\n",
       " ('пикирование_NOUN', 0.49755290150642395),\n",
       " ('стефановский_PROPN', 0.49500828981399536),\n",
       " ('арцеулов_PROPN', 0.4792757034301758),\n",
       " ('парашют_VERB', 0.47498655319213867),\n",
       " ('траектория_NOUN', 0.4714280068874359),\n",
       " ('винт_NOUN', 0.4686664342880249),\n",
       " ('спираль_NOUN', 0.467825323343277)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.most_similar(positive=['штопор_NOUN'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adiAA7AqIY4g"
   },
   "source": [
    "По аналогии с Италия -- пицца, Сибирь -- пельмень, придумайте похожую связку слов для проверки: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "u5bzD-ZXKOSY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('клюв_NOUN', 0.5999110341072083)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.most_similar(positive=['птица_NOUN', 'нос_NOUN'], negative=['человек_NOUN'])[:5][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpIDtQIzKO1M"
   },
   "source": [
    "Приведите пример трех слов w1, w2, w3, таких, что w1 и w2 являются синонимами, w1 и w3 являются антонимами, но при этом, similarity(w1, w2) < similarity(w1, w3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XFXaxVhlKPb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2, w3 = 'веселый_ADJ', 'улыбчивый_ADJ', 'грустный_ADJ'\n",
    "model_ru.similarity(w1, w2) < model_ru.similarity(w1, w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rfihwVJKwzC"
   },
   "source": [
    "### Задание\n",
    "\n",
    "Напишите функцию, которая принимает на вход предложение, и заменяет случайное существительное в нём на \"ассоциат\" -- ближайшее к нему слово из модели word2vec.\n",
    "\n",
    "NB: для этого вам понадобится морфологический анализатор. Советую использовать pymorphy (мы кратко говорили про него на прошлом семинаре)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCMPmoRrM3ES"
   },
   "source": [
    " как пользоваться pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "BHf_0AeCKxIs",
    "outputId": "e026df1b-756c-469e-ca9b-cdd6d88b457d"
   },
   "outputs": [],
   "source": [
    "# !pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "analyser = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "xUvP-U-nNF6Y",
    "outputId": "52621d20-d246-4795-9b65-0129cefe0150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='слово', score=0.615384, methods_stack=((<DictionaryAnalyzer>, 'слово', 54, 0),)),\n",
       " Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='слово', score=0.384615, methods_stack=((<DictionaryAnalyzer>, 'слово', 54, 3),))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разобрать слово (в данном случае возможно два разбора, поэтому получаем список из двух элементов)\n",
    "result = analyser.parse('слово')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "enrQF-SQOtQP",
    "outputId": "f21bf374-56a6-45fa-89a1-368906d9a797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# достать часть речи\n",
    "result[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "a8P6VpjDPR_z",
    "outputId": "b1d91898-db84-4405-d7c7-38ca3b02ea87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'слову'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# поставить в дательный падеж\n",
    "result[0].inflect(frozenset(['datv'])).word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNhjJC-xPwMg"
   },
   "source": [
    "Ваша функция (для простоты можно не пытаться поставить слово в \"нужную\" форму и ограничиться именительным падежом):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import pymorphy2\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Initialize pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Кот преследовал мышь по всему двору  до вечера.\n",
      "Output Sentence: кошка преследовал крысу по всему сараю до утра .\n"
     ]
    }
   ],
   "source": [
    "def inflect_word_to_case(word, target_word):\n",
    "    '''Inflect a word to the case of another word'''\n",
    "    target_parsed = morph.parse(target_word)[0]\n",
    "    word_parsed = morph.parse(word)[0]\n",
    "    inflected_form = word_parsed.inflect({target_parsed.tag.case})\n",
    "    \n",
    "    if inflected_form:\n",
    "        return inflected_form.word\n",
    "    else:\n",
    "        return word  # Return the original word if inflection is not possible\n",
    "\n",
    "\n",
    "def replace_noun_with_associate(sentence, word2vec_model, sim_rank=1):\n",
    "    \"\"\"Replace a random noun in the sentence with a similar word from Word2Vec\"\"\"\n",
    "\n",
    "    words = word_tokenize(sentence)    # Split sentence into list of words\n",
    "    new_words = []\n",
    "\n",
    "    for word in words:  # Iterate over each word of the sentence\n",
    "        parsed = analyser.parse(word)[0]    # Parse the word with pymorphy2\n",
    "        if 'NOUN' in parsed.tag:        # If the word is a noun\n",
    "            # Find the most similar word from the vocabulary\n",
    "            word2replace = parsed.normal_form + '_' + 'NOUN'\n",
    "            similar_words = word2vec_model.most_similar(positive=[word2replace], topn=sim_rank)\n",
    "            if similar_words:   # If there's a similar word, replace it\n",
    "                closest_word = similar_words[sim_rank-1][0].rsplit('_', 1)[0]  # Remove the UPOS tag \n",
    "                closest_word = inflect_word_to_case(closest_word, word)  # Inflect the word to the case of the original word\n",
    "                new_words.append(closest_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Example sentence\n",
    "input_sentence = \"Кот преследовал мышь по всему двору  до вечера.\"\n",
    "\n",
    "# Replace a random noun with a similar word from Word2Vec\n",
    "output_sentence = replace_noun_with_associate(input_sentence, model_ru)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Output Sentence:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ехал медведь на велосипеде и упал в лужу.\n",
      "\n",
      "Output Sentence: Ехал медведь на мотоцикле и упал в лужу .\n",
      "Output Sentence: Ехал медведь на трехколёсном и упал в лужицу .\n",
      "Output Sentence: Ехал зверь на мотороллере и упал в жижу .\n",
      "Output Sentence: Ехал волк на самокате и упал в грязь .\n",
      "Output Sentence: Ехал медведица на велосипедисте и упал в асфальт .\n",
      "Output Sentence: Ехал барсук на велосипедном и упал в мокрого .\n",
      "Output Sentence: Ехал тигр на мопеде и упал в тротуар .\n",
      "Output Sentence: Ехал заяц на автомобиле и упал в канаву .\n",
      "Output Sentence: Ехал кабан на коньке и упал в выбоину .\n",
      "Output Sentence: Ехал медвежонок на двухколёсном и упал в колдобину .\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "input_sentence = \"Ехал медведь на велосипеде и упал в лужу.\\n\"\n",
    "print(f\"Input Sentence: {input_sentence}\")\n",
    "# Replace a random noun with a similar word from Word2Vec\n",
    "for sim_rank in range(1, 11):\n",
    "    output_sentence = replace_noun_with_associate(input_sentence, model_ru, sim_rank=sim_rank)\n",
    "    print(f\"Output Sentence: {output_sentence}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNHMtmbqX-rl"
   },
   "source": [
    "## Как обучить свою модель\n",
    "\n",
    "В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-1Vz9le-X-rl",
    "outputId": "698576f8-edab-43c5-ee3a-0c2c295fefa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv\n",
    "\n",
    "data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIZaQ3kYX-rt"
   },
   "source": [
    "Убираем из данных ссылки, html-разметку и небуквенные символы, а затем приводим все к нижнему регистру и токенизируем. На выходе получается массив из предложений, каждое из которых представляет собой массив слов. Здесь используется токенизатор из библиотеки `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZAzxyk3WTgrd"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "irbunSOfX-rt"
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False ):\n",
    "    # убираем ссылки\n",
    "    review_text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    # достаем сам текст\n",
    "#     review_text = BeautifulSoup(review_text, \"lxml\").get_text()\n",
    "    # оставляем только буквенные символы\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords: # убираем стоп-слова\n",
    "        stops = stopwords.words(\"english\")\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # разбиваем обзор на предложения\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # применяем предыдущую функцию к каждому предложению\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "bolPEuQgX-rv",
    "outputId": "9a9a76b4-5d61-46ce-a72c-c25f2bbda75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:17<00:00, 2848.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []  \n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "for review in tqdm(data[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "FbrH6TdfX-rx",
    "outputId": "2122347d-0eaf-4cb4-c025-e96976360ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529416\n",
      "['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Cqb_NOteX-rz"
   },
   "outputs": [],
   "source": [
    "# это понадобится нам позже\n",
    "\n",
    "with open('clean_text.txt', 'w') as f:\n",
    "    # for s in sentences[:5000]:\n",
    "    for s in sentences:        \n",
    "        f.write(' '.join(s))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqFhuv7XX-r1"
   },
   "source": [
    "Обучаем и сохраняем модель. \n",
    "\n",
    "\n",
    "Основные параметры:\n",
    "* данные должны быть итерируемым объектом \n",
    "* size — размер вектора, \n",
    "* window — размер окна наблюдения,\n",
    "* min_count — мин. частотность слова в корпусе,\n",
    "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
    "* sample — порог для downsampling'a высокочастотных слов,\n",
    "* workers — количество потоков,\n",
    "* alpha — learning rate,\n",
    "* iter — количество итераций,\n",
    "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение превышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
    "\n",
    "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "OB2FqIioX-r1",
    "outputId": "7a8840a9-a12b-4247-bf6b-b02c767ce2c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 2min 54s, sys: 2.07 s, total: 2min 56s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_en = word2vec.Word2Vec(sentences, workers=4, vector_size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFU6SYAZX-r3"
   },
   "source": [
    "Смотрим, сколько в модели слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HA3P0CUYX-r4",
    "outputId": "1bfec39d-aa30-4a4e-b77c-e8ed9c8c2571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28316\n"
     ]
    }
   ],
   "source": [
    "print(len(model_en.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mU9sbGYX-r8"
   },
   "source": [
    "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "n8OQxmUyX-r9",
    "outputId": "4e4ab20e-31e4-4c02-fdfa-61b4ba8be133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 0.7513849139213562)]\n",
      "[('men', 0.6290287971496582)]\n",
      "[('europe', 0.7354639768600464), ('uk', 0.7176207304000854), ('germany', 0.7155715823173523)]\n",
      "novel\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
    "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
    "\n",
    "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
    "\n",
    "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juR4NX0wX-r_"
   },
   "source": [
    "### Как дообучить существующую модель\n",
    "\n",
    "При тренировке модели \"с нуля\" веса инициализируются случайно, однако можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
    "\n",
    "Сначала посмотрим близость какой-нибудь пары слов в имеющейся модели, чтобы потом сравнить результат с дообученной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rAUVp96kX-sA",
    "outputId": "610c4080-a724-40e5-dc77-30f83bd46cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31098974"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.wv.similarity('lion', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxsz8DKMX-sD"
   },
   "source": [
    "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through', 'the', 'looking-glass', 'by', 'lewis', 'carroll', 'chapter', 'i', 'looking-glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', '', 'it', 'was', 'the', 'black', 'kitten’s', 'fault', 'entirely'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldn’t', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# убираем переносы строк, токенизируем текст\n",
    "text = re.sub('\\n', ' ', text)\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "# убираем всю пунктуацию и делим текст на слова по пробелу\n",
    "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
    "clean_sents = []\n",
    "for sent in sents:\n",
    "    s = [w.lower().strip(punct) for w in sent.split()]\n",
    "    clean_sents.append(s)\n",
    "    \n",
    "print(clean_sents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH8f7GNBX-sK"
   },
   "source": [
    "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
    "\n",
    "**NB!** Дообучить можно только полную модель, а `KeyedVectors` — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Rv--44TmX-sL",
    "outputId": "0973b4a0-dcbe-498e-86a5-28f6e9673aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"movie_reviews.model\" # save the model with different name\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "f6eZwBQzX-sQ",
    "outputId": "be412d2b-8cc7-424b-9251-bc2aedb1474d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96943, 150225)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(model_path) # load model\n",
    "\n",
    "model.build_vocab(clean_sents, update=True) # update vocabulary\n",
    "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)   # train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMLhISWiX-sS"
   },
   "source": [
    "Лев и кролик стали чуть ближе друг к другу!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "4USaUjmzX-sS",
    "outputId": "5f20a83c-b50e-4ca3-bd28-ddb8e16e319e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('lion', 'rabbit') > model_en.wv.similarity('lion', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5bYd1qmX-sT"
   },
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать. Здесь используется L2-нормализация: вектора нормализуются так, что если сложить квадраты всех элементов вектора, в сумме получится 1. \n",
    "\n",
    "Кроме того, сохраним не полные вектора, а `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "ZAVaTYvQX-sU",
    "outputId": "4c2ef602-996f-4ef0-9a77-55a53cabccb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/qq8zb5hx0w9cgzgkf9_v2sh00000gq/T/ipykernel_82450/444224005.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)   # normalize vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)   # normalize vectors\n",
    "model_path = \"movies_alice.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "# save the first model as binary file\n",
    "model_en.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXKnPffPX-sY"
   },
   "source": [
    "## Оценка\n",
    "\n",
    "Это, конечно, хорошо, но как понять, какая модель лучше? Или вот, например, я сделал свою модель, а как понять, насколько она хорошая?\n",
    "\n",
    "Для этого существуют специальные датасеты для оценки качества дистрибутивных моделей. Основных два: один измеряет точность решения задач на аналогии (про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. \n",
    "\n",
    "### Word Similarity\n",
    "\n",
    "Этот метод заключается в том, чтобы оценить, насколько представления о семантической близости слов в модели соотносятся с \"представлениями\" людей.\n",
    "\n",
    "| слово 1    | слово 2    | близость | \n",
    "|------------|------------|----------|\n",
    "| кошка      | собака     | 0.7      |  \n",
    "| чашка      | кружка     | 0.9      |       \n",
    "\n",
    "Для каждой пары слов из заранее заданного датасета мы можем посчитать косинусное расстояние, и получить список таких значений близости. При этом у нас уже есть список значений близостей, сделанный людьми. Мы можем сравнить эти два списка и понять, насколько они похожи (например, посчитав корреляцию). Эта мера схожести должна говорить о том, насколько модель хорошо моделирует расстояния до слова.\n",
    "\n",
    "### Аналогии\n",
    "\n",
    "Другая популярная задача для \"внутренней\" оценки называется задачей поиска аналогий. Как мы уже разбирали выше, с помощью простых арифметических операций мы можем модифицировать значение слова. Если заранее собрать набор слов-модификаторов, а также слов, которые мы хотим получить в результаты модификации, то на основе подсчёта количества \"попаданий\" в желаемое слово мы можем оценить, насколько хорошо работает модель.\n",
    "\n",
    "В качестве слов-модификаторов мы можем использовать семантические аналогии. Скажем, если у нас есть некоторое отношение \"страна-столица\", то для оценки модели мы можем использовать пары наподобие \"Россия-Москва\", \"Норвегия-Осло\", и т.д. Датасет будет выглядеть следующм образом:\n",
    "\n",
    "| слово 1    | слово 2    | отношение     | \n",
    "|------------|------------|---------------|\n",
    "| Россия     | Москва     | страна-столица|  \n",
    "| Норвегия   | Осло       | страна-столица|\n",
    "\n",
    "Рассматривая случайные две пары из этого набора, мы хотим, имея триплет (Россия, Москва, Норвегия) хотим получить слово \"Осло\", т.е. найти такое слово, которое будет находиться в том же отношении со словом \"Норвегия\", как \"Россия\" находится с Москвой. \n",
    "\n",
    "Датасеты для русского языка можно скачать на странице с моделями на RusVectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": capital-common-countries\n",
      "афины_S греция_S багдад_S ирак_S\n",
      "афины_S греция_S бангкок_S таиланд_S\n",
      "афины_S греция_S пекин_S китай_S\n",
      "афины_S греция_S берлин_S германия_S\n",
      "афины_S греция_S берн_S швейцария_S\n",
      "афины_S греция_S каир_S египет_S\n",
      "афины_S греция_S канберра_S австралия_S\n",
      "афины_S греция_S ханой_S вьетнам_S\n",
      "афины_S греция_S гавана_S куба_S\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt\n",
    "!head ru_analogy_tagged.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qZcF3nGX-sp"
   },
   "source": [
    "## FastText\n",
    "\n",
    "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грам. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
    "\n",
    "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
    "* [Сайт](https://fasttext.cc/)\n",
    "* [Тьюториал](https://fasttext.cc/docs/en/support.html)\n",
    "* [Вектора для 157 языков](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "* [Вектора, обученные на википедии](https://fasttext.cc/docs/en/pretrained-vectors.html) (отдельно для 294 разных языков)\n",
    "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
    "\n",
    "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "NvzM2AL7Z8gl",
    "outputId": "a93516dc-c859-41f7-e922-2ebd06eadd6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 12M words\n",
      "Number of words:  39721\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   21284 lr:  0.000000 avg.loss:  2.201756 ETA:   0h 0m 0s0h 4m23s 11.3% words/sec/thread:   14613 lr:  0.044337 avg.loss:  2.242726 ETA:   0h 4m15sm 2s 29.4% words/sec/thread:   15932 lr:  0.035322 avg.loss:  2.242181 ETA:   0h 3m 6s 31.8% words/sec/thread:   16147 lr:  0.034123 avg.loss:  2.241171 ETA:   0h 2m57s 34.0% words/sec/thread:   16084 lr:  0.032984 avg.loss:  2.239221 ETA:   0h 2m52s  16210 lr:  0.029984 avg.loss:  2.235671 ETA:   0h 2m35s 40.3% words/sec/thread:   16222 lr:  0.029873 avg.loss:  2.235320 ETA:   0h 2m34s 40.6% words/sec/thread:   16231 lr:  0.029705 avg.loss:  2.235434 ETA:   0h 2m33s\n"
     ]
    }
   ],
   "source": [
    "# ! git clone https://github.com/facebookresearch/fastText.git\n",
    "# ! pip3 install fastText/.\n",
    "import fasttext\n",
    "\n",
    "# Train an unsupervised model and return a model object. \n",
    "# Input must be a filepath. The input text does not need to be tokenized as per \n",
    "# the tokenize function, but it must be preprocessed and encoded as UTF-8.\n",
    "ft_model = fasttext.train_unsupervised('clean_text.txt', minn=3, maxn=4, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NNiuzf4DX-sr",
    "outputId": "944550f0-33d5-498b-f16d-7b9872667338"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05911254,  0.02789777, -0.01386697, -0.129203  , -0.00770547,\n",
       "       -0.01958451,  0.06880166,  0.23435326, -0.07443594, -0.16534059],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.get_word_vector(\"movie\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "TsZnXqH8X-st",
    "outputId": "983fcc63-9a13-43fb-a4be-3dfd55d70863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6737564206123352, 'actors'),\n",
       " (0.5949331521987915, 'tractor'),\n",
       " (0.5653902888298035, 'ctor'),\n",
       " (0.5475830435752869, 'reactor'),\n",
       " (0.53066086769104, 'actress'),\n",
       " (0.49728068709373474, 'performance'),\n",
       " (0.4859069585800171, 'talented'),\n",
       " (0.47997209429740906, 'lector'),\n",
       " (0.47709763050079346, 'performer'),\n",
       " (0.4746330678462982, 'role')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.get_nearest_neighbors('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "ttAgMPC_X-sx",
    "outputId": "9bbec1ad-d83a-4e75-df98-94356598a671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7664213180541992, 'actress'),\n",
       " (0.7175777554512024, 'actresses'),\n",
       " (0.6524512767791748, 'actors'),\n",
       " (0.6201873421669006, 'actor'),\n",
       " (0.5056630373001099, 'talented'),\n",
       " (0.4900783598423004, 'acte'),\n",
       " (0.484948992729187, 'acting'),\n",
       " (0.47762584686279297, 'performers'),\n",
       " (0.47757741808891296, 'juhi'),\n",
       " (0.4595927596092224, 'ehsaan')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проблема с опечатками решена\n",
    "\n",
    "ft_model.get_nearest_neighbors('actr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "V9alEdz0X-s0",
    "outputId": "1126182a-8abd-471c-9346-59e852625c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6338699460029602, 'moviegoing'),\n",
       " (0.6033056378364563, 'movie'),\n",
       " (0.6021876335144043, 'geek'),\n",
       " (0.6003331542015076, 'moviegoer'),\n",
       " (0.5785683393478394, 'mfj'),\n",
       " (0.5781276226043701, 'movies'),\n",
       " (0.566527247428894, 'moviegoers'),\n",
       " (0.563457190990448, 'sb'),\n",
       " (0.5476100444793701, 'hpl'),\n",
       " (0.5433463454246521, 'beek')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проблема с out of vocabulary словами - тоже\n",
    "\n",
    "ft_model.get_nearest_neighbors('moviegeek')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "0f5FRfEMX-s3"
   },
   "source": [
    "Кроме этого, fastText можно использовать для классификации, для этого нужен следующий формат размеченных данных:\n",
    "\n",
    "__label_1__  text_1\n",
    "\n",
    "__label_2__  text_2\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/velo1/SynologyDrive/GIT_syno/data/NLP/tweets_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>мыс на меня обиделась:(\\nя ей даже ничего не с...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аааааааааааааааааааа,не хочу на работу :(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У меня какой-то особенный вид ушей! :D, некото...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@simonovkon  он неплохой человек в жизни. Я ра...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Darina_Lo: Домааааа\\nЕхали на такси, пели ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  мыс на меня обиделась:(\\nя ей даже ничего не с...  negative\n",
       "1          аааааааааааааааааааа,не хочу на работу :(  negative\n",
       "2  У меня какой-то особенный вид ушей! :D, некото...  positive\n",
       "3  @simonovkon  он неплохой человек в жизни. Я ра...  negative\n",
       "4  RT @Darina_Lo: Домааааа\\nЕхали на такси, пели ...  positive"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-YB6B4CvmNJ8",
    "outputId": "4f50acfa-27c4-4a50-ddcb-612fd5cfc85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226834"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QuPeLrvX-tI"
   },
   "source": [
    "Запишем полученные данные в формате для обучения классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "AGOFQSekX-tI",
    "outputId": "0eba65f3-d4b1-4b38-e31a-b9f1859e61ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 151978\n",
      "total test examples 74856\n"
     ]
    }
   ],
   "source": [
    "X = df.text.tolist()\n",
    "y = df.label.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "mogTwRfnX-tK"
   },
   "outputs": [],
   "source": [
    "with open('data.train.txt', 'w+') as outfile:\n",
    "    # format train data for fasttext classifier\n",
    "    for i in range(len(X_train)):\n",
    "        outfile.write('__label__' + y_train[i] + ' '+ X_train[i] + '\\n')\n",
    "    \n",
    "\n",
    "with open('test.txt', 'w+') as outfile:\n",
    "    # format test data for fasttext classifier\n",
    "    for i in range(len(X_test)):\n",
    "        outfile.write('__label__' + y_test[i] + ' ' + X_test[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "u0rt2mhjX-tL",
    "outputId": "fee049fd-e505-483b-970f-3e022acaad38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  422367\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  887760 lr:  0.000000 avg.loss:  0.213438 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.8358982579886716\n",
      "R@1: 0.8358982579886716\n",
      "Number of examples: 74856\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.train_supervised('data.train.txt')\n",
    "result = classifier.test('test.txt')\n",
    "\n",
    "print('P@1:', result[1])    # Precision at Recall one\n",
    "print('R@1:', result[2])    # Recall at Precision one\n",
    "print('Number of examples:', result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkb-ckInX-tM"
   },
   "source": [
    "## Задание\n",
    "\n",
    "1. Мы будем работать с данными fakenews отсюда: https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv\n",
    "2. Проведите препроцессинг текста. Разбейте данные на train и test для задачи классификации.\n",
    "3. Обучите свою модель w2v (или возьмите любую подходящую предобученную модель). Реализуйте функцию для вычисления вектора текста. \n",
    "4. Обучите на полученных средних векторах алгоритм классификации.\n",
    "\n",
    "Бонус: Модифицируйте функцию вычисления среднего вектора: взвешивайте вектора слов соответствующими весами tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "bLJXu8KKX-tM",
    "outputId": "ba48f480-2d13-4c3a-a6d7-9378a723a2a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv\n",
    "df = pd.read_csv('Constraint_Train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6420, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 924/6420 [00:00<00:01, 4457.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6420/6420 [00:01<00:00, 4930.58it/s]\n"
     ]
    }
   ],
   "source": [
    "corpora = [word_tokenize(text.lower()) for text in tqdm(df['tweet'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x182439ed0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tweets = Word2Vec(corpora, min_count=2, vector_size=200, workers=4, window=10, sg=1) # initialize model\n",
    "model_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donald', 0.9530353546142578),\n",
       " ('trump', 0.9352554678916931),\n",
       " ('obama', 0.9114581942558289),\n",
       " ('barack', 0.9011787176132202),\n",
       " ('former', 0.8968438506126404),\n",
       " ('joe', 0.8864171504974365),\n",
       " ('downplayed', 0.8814111351966858),\n",
       " ('tweeted', 0.8778067827224731),\n",
       " ('pelosi', 0.8752232789993286),\n",
       " ('duterte', 0.8705849647521973)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tweets.wv.most_similar('president')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_embedding(tweet):\n",
    "    '''Returns the embedding of a text as the SUM of the embeddings of its words.'''\n",
    "    \n",
    "    tokens = [word for word in tweet]\n",
    "    embedding = np.zeros((model_tweets.vector_size,), dtype=np.float32)\n",
    "    for token in tokens:\n",
    "        if token in model_tweets.wv:\n",
    "            embedding += model_tweets.wv[token]\n",
    "            \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [get_tweet_embedding(tweet) for tweet in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), 6420)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, df['label'].to_list(), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=2000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.91      0.93      0.92      1004\n",
      "        real       0.93      0.92      0.93      1115\n",
      "\n",
      "    accuracy                           0.92      2119\n",
      "   macro avg       0.92      0.93      0.92      2119\n",
      "weighted avg       0.93      0.92      0.92      2119\n",
      "\n",
      "[[ 932   72]\n",
      " [  87 1028]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.91      0.92      1004\n",
      "        real       0.92      0.94      0.93      1115\n",
      "\n",
      "    accuracy                           0.93      2119\n",
      "   macro avg       0.93      0.93      0.93      2119\n",
      "weighted avg       0.93      0.93      0.93      2119\n",
      "\n",
      "[[ 915   89]\n",
      " [  64 1051]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Sv0wxUXIX-se"
   ],
   "name": "embeddings_sem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
